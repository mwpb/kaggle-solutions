# Unsupervised learning: seeking representations of the data

[website](https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html)

## Clustering: grouping observations together

Aim: split the data into well-separated partitions called clusters.

As an application, after clustering we can compress data by replacing each observation with the man of its cluster.
(The website gives a 'posterisation' example.)

### K-means clustering

Finds k clusters defined by the property:
each observation falls into the cluster whose means is closest.
(This is in general a hard problem as one considers all clusters.
However there are heuristic algorithms that converge to a local extreme point.)

In sklearn we use:
```
from sklearn import cluster
k_means = cluster.KMeans(n_clusters = 3)
k_means.fit(X)
```

### Hierarchical clustering

In general these types of clustering fall into one of two categories:-
* agglomerative clustering (bottom-up):
	* each observation starts in its own cluster
	* then clusters are merged to minimise a 'linkage' condition
* divisive clustering (top-down):
	* all observations start in the same cluster
	* iteratively splits into more clusters
which means that top-down is better for fewer clusters and vice-versa.

#### Connectivity constrained clustering

Given an image we define:-
* what it means for two pixels to be similar
* what is means for two pixels to be neighbouring
and then create equivalence classes generated by identifying similar neighbours.

In sklearn the similarity is handled by the clustering class being used.
In addition we can add connectivity constraints to prevent undesirable clustering.
(It may be undesirable to cluster far away points on an image for example.)

```
from sklearn.feature_extraction.image import grid_to_graph
cty = grid_to_graph(x_dim, y_dim, ?z_dim)
```

and then this is passed into a clustering class for instance:

```
from sklearn.cluster import AgglomerativeClustering
ac = AgglomerativeClustering(n_clusters = 5, connectivity = cty)
```

to constrain the clustering algorithm.

#### Feature agglomeration

It is also possible to perform clustering on the features.
This is useful if one wants to avoid the 'curse of dimensionality'.

In sklearn the interface is similar:
```
from sklearn.cluster import FeatureAgglomeration
agglo = cluster.FeatureAgglomeration(n_clusters=32, connectivity = cty)
```

